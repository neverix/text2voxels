{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmVyC3Ba0J3s"
      },
      "source": [
        "# text2voxels v0.2.1\n",
        "\n",
        "by [@nev#4905](https://twitter.neverix.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VWGmqcuMXJCt"
      },
      "outputs": [],
      "source": [
        "#@title mount Google Drive\n",
        "from google.colab import drive\n",
        "do_it = True  #@param {type: \"boolean\"}\n",
        "if do_it:\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "saF7Y4zE1ECY"
      },
      "outputs": [],
      "source": [
        "#@title create CLIP\n",
        "!pip install torch_optimizer > /dev/null 2> /dev/null\n",
        "!pip install clip-anytorch > /dev/null 2> /dev/null\n",
        "import clip\n",
        "\n",
        "\n",
        "clip_version = \"ViT-B/16\"  #@param [\"ViT-B/16\", \"ViT-B/32\", \"RN50\", \"RN50x4\"] {type: \"string\", allow-input: true}\n",
        "model, preprocess = clip.load(clip_version, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uzKgUwaP1ECZ"
      },
      "outputs": [],
      "source": [
        "#@title noise\n",
        "#@markdown like in the dream fields paper\n",
        "\n",
        "#@markdown stolen from https://gist.github.com/ac1b097753f217c5c11bc2ff396e0a57\n",
        "\n",
        "# ported from https://github.com/pvigier/perlin-numpy/blob/master/perlin2d.py\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "#@markdown lowering to 0.2 sometimes improves the results\n",
        "noise_level = 0.5  #@param {type: \"number\"}\n",
        "\n",
        "\n",
        "def rand_perlin_2d(shape, res, fade = lambda t: 6*t**5 - 15*t**4 + 10*t**3):\n",
        "    delta = (res[0] / shape[0], res[1] / shape[1])\n",
        "    d = (shape[0] // res[0], shape[1] // res[1])\n",
        "    \n",
        "    grid = torch.stack(torch.meshgrid(torch.arange(0, res[0], delta[0]), torch.arange(0, res[1], delta[1])), dim = -1) % 1\n",
        "    angles = 2*math.pi*torch.rand(res[0]+1, res[1]+1)\n",
        "    gradients = torch.stack((torch.cos(angles), torch.sin(angles)), dim = -1)\n",
        "    \n",
        "    tile_grads = lambda slice1, slice2: gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]].repeat_interleave(d[0], 0).repeat_interleave(d[1], 1)\n",
        "    dot = lambda grad, shift: (torch.stack((grid[:shape[0],:shape[1],0] + shift[0], grid[:shape[0],:shape[1], 1] + shift[1]  ), dim = -1) * grad[:shape[0], :shape[1]]).sum(dim = -1)\n",
        "    \n",
        "    n00 = dot(tile_grads([0, -1], [0, -1]), [0,  0])\n",
        "    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])\n",
        "    n01 = dot(tile_grads([0, -1],[1, None]), [0, -1])\n",
        "    n11 = dot(tile_grads([1, None], [1, None]), [-1,-1])\n",
        "    t = fade(grid[:shape[0], :shape[1]])\n",
        "    return math.sqrt(2) * torch.lerp(torch.lerp(n00, n10, t[..., 0]), torch.lerp(n01, n11, t[..., 0]), t[..., 1])\n",
        "\n",
        "def rand_perlin_2d_octaves(shape, res, octaves=1, persistence=0.5):\n",
        "    noise = torch.zeros(shape)\n",
        "    frequency = 1\n",
        "    amplitude = 1\n",
        "    for _ in range(octaves):\n",
        "        noise += amplitude * rand_perlin_2d(shape, (frequency*res[0], frequency*res[1]))\n",
        "        frequency *= 2\n",
        "        amplitude *= persistence\n",
        "    noise *= random.random() - noise_level  # haha\n",
        "    noise += random.random() - noise_level  # haha x2\n",
        "    return noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-eByXpN5-pz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title does something\n",
        "import gc\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from math import ceil\n",
        "import torch_optimizer\n",
        "from base64 import b64encode\n",
        "from ipywidgets import Output\n",
        "from IPython.display import HTML\n",
        "from more_itertools import chunked\n",
        "from tqdm.auto import trange, tqdm\n",
        "from subprocess import Popen, PIPE\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model.requires_grad_(False)\n",
        "#@markdown set to zero for no seed\n",
        "seed =   24#@param {type: \"integer\"}\n",
        "#@markdown ## video settings\n",
        "#@markdown image size, must be divisible by 28\n",
        "w =   224#@param {type: \"integer\"}\n",
        "#@markdown image size for CLIP, can't be bigger than w. 252 also only works with ViT-B/32\n",
        "cutout_w =   \"224\"#@param [224, 252] {type: \"string\"}\n",
        "#@markdown frames per second\n",
        "fps = 15  #@param {type: \"integer\"}\n",
        "#@markdown if empty, make the name of the output video the same as the text\n",
        "out_path = \"\"  #@param {type: \"string\"}\n",
        "#@markdown download the resulting video automatically\n",
        "download = False  #@param {type: \"boolean\"}\n",
        "#@markdown ## text\n",
        "#@markdown prompts with capital letters work better\n",
        "text = \"a black swan\"  #@param [\"dream landscape by Van Gogh\", \"a black swan\", \"a flying whale\", \"a white rabbit\", \"a beautiful bonsai tree\", \"beautiful pine trees\", \"a cute creature\", \"chtulhu is watching\", \"a frog\", \"a robot dog. a robot in the shape of a dog\", \"a carrot\", \"personal computer\", \"golden pyramid\", \"golden snitch\", \"a new planet ruled by ophanims\", \"a blue cat\", \"a burning potato\", \"an eggplant\", \"a purple mouse\", \"a ninja\", \"a green teapot\", \"a black rabbit\", \"an avocado armchair\", \"smoking a joint\", \"3D old-school telephone\", \"a wooden chair\", \"spider-man figure\", \"a 3D monkey #pixelart\", \"a minecraft grass block\", \"minecraft creeper\", \"minecraft landscape\", \"a shining star\"] {allow-input: true}\n",
        "#@markdown capitalize the first character automatically?\n",
        "capitalize = True  #@param {type: \"boolean\"}\n",
        "#@markdown capitalize every word automatically?\n",
        "capitalize_words = False  #@param {type: \"boolean\"}\n",
        "#@markdown capitalize every letter after a number (3d -> 3D)\n",
        "capitalize_number = True  #@param {type: \"boolean\"}\n",
        "#@markdown rename the output video if it exists\n",
        "rename_out = True  #@param {type: \"boolean\"}\n",
        "#@markdown ## learning settings\n",
        "#@markdown load a checkpoint saved before, leave empty for no checkpoint\n",
        "load_checkpoint = \"\"  #@param {type: \"string\"}\n",
        "#@markdown load checkpoints from google drive\n",
        "load_gdrive = False  #@param {type: \"boolean\"}\n",
        "#@markdown ignore file not found error and start from scratch if the checkpoint file was not found\n",
        "ignore_err = True  #@param {type: \"boolean\"}\n",
        "#@markdown number of steps to train for\n",
        "train_steps =    720#@param {type: \"integer\"}\n",
        "#@markdown stop training after `time_stop` seconds. negative values are ignored\n",
        "\n",
        "#@markdown stops after two minutes by default.\n",
        "\n",
        "#@markdown you can stop the generation process manually\n",
        "time_stop =    -1#@param {type: \"number\"}\n",
        "#@markdown optimizationparameters\n",
        "\n",
        "#@markdown increasing the batch size or gradient accumulation makes the results a little smoother and gets rid of duplicates\n",
        "\n",
        "#@markdown training runs with gradient accumulation (1-2) look better\n",
        "grad_acc = 2#@param {type: \"integer\"}\n",
        "train_batch =   2#@param {type: \"integer\"}\n",
        "#@markdown learning rate, decreasing it makes the optimization slower but slightly more detailed, anything below 1e-2 is too slow\n",
        "lr =   1e-1#@param {type: \"number\"}\n",
        "#@markdown FP16, reduces memory usage but not fully tested\n",
        "fp16 = False  #@param {type: \"boolean\"}\n",
        "#@markdown optimizer name, you can use any from the library [torch-optimizer](https://github.com/jettify/pytorch-optimizer)\n",
        "\n",
        "#@markdown these might use more memory, so decrease rendering steps and batch size if an error appears\n",
        "\n",
        "#@markdown the optimizers are ordered by performance decreasing\n",
        "\n",
        "#@markdown when using Adam, increase the gradient accumulation to ~8\n",
        "\n",
        "#@markdown MADGRAD is pretty good but uses more memory\n",
        "optimizer_name = \"MADGRAD\"  #@param [\"Adam\", \"MADGRAD\"] {type: \"string\", allow-input: true}\n",
        "#@markdown ## rendering settings\n",
        "#@markdown size of the box\n",
        "extent =   1#@param {type: \"number\"}  # 1.28 # 1.3  # 1  # 1.5\n",
        "#@markdown bilinear interpolation (uses 8x more memory, makes slightly smoother images)\n",
        "use_weights = False  #@param {type: \"boolean\"}\n",
        "#@markdown near and far planes for the camera\n",
        "near =  1#@param {type: \"number\"}\n",
        "far =   5#@param {type: \"number\"}\n",
        "#@markdown size of the FOV plane. 1 for 90 degrees, more for more\n",
        "fov_plane =  0.75 #@param {type: \"number\"}\n",
        "#@markdown side of the box. 128 is 4x faster, but it might be lower quality\n",
        "block_size = 256\n",
        "#@markdown density at the edges\n",
        "mask_value = 0  #@param {type: \"number\"}\n",
        "#@markdown default camera offset and angle\n",
        "offset =   3#@param {type: \"number\"}\n",
        "angle = 0  #@param {type: \"number\"}\n",
        "#@markdown starting scale for the pyramid\n",
        "scale_from =   1#@param {type: \"integer\"}\n",
        "#@markdown how much the scale grows (exponentially)\n",
        "scale_decay = 0.25  #@param {type: \"number\"}  # 0.5\n",
        "#@markdown raise the scale while training? (not implemented)\n",
        "scale_schedule = False  #@param  {type: \"boolean\"}\n",
        "#@markdown initialization density\n",
        "init_density = 0.05  #@param {type: \"number\"}\n",
        "#@markdown initialization type (what the cube looks like in the beginning)\n",
        "init_type = \"uniform\"  #@param [\"uniform\", \"random\", \"spherical\"] {type: \"string\"}\n",
        "#@markdown (for spherical init) the power that the init is raised to\n",
        "init_pow = 1  #@param {type: \"number\"}\n",
        "#@markdown number of raycasting steps. raising this improves the resolution but makes the renderer use more memory\n",
        "rendering_steps = 100  #@param {type: \"integer\"}\n",
        "#@markdown background color\n",
        "bg_color = 0.95  #@param {type: \"number\"}\n",
        "#@markdown grayscale rendering\n",
        "grayscale = False  #@param {type: \"boolean\"}\n",
        "#@markdown cutoff density for export and sparse rendering\n",
        "quantize_thresh = 0.2 #@param {type: \"number\"}\n",
        "#@markdown palette image URL or local path\n",
        "\n",
        "#@markdown colors will be extracted from this and used for the voxels\n",
        "\n",
        "#@markdown warning: palettes slow down generation by around 2 times.\n",
        "palette_path = \"\"  #@param [\"\", \"https://cdn.pixabay.com/photo/2019/07/03/01/55/black-swan-4313444_1280.jpg\", \"https://pixahive.com/wp-content/uploads/2020/12/A-car-in-a-desert-236323-pixahive.jpg\"] {type: \"string\", allow-input: true}\n",
        "#@markdown number of colors for the palette\n",
        "palette_colors =  6#@param {type: \"integer\"}\n",
        "#@markdown ## objective settings\n",
        "#@markdown the image prompt URL or local path. make blank if you don't want image prompting\n",
        "img_path = \"\"  #@param {type: \"string\"}\n",
        "#@markdown similarity to the image prompt, ignored if you use a blank image URL\n",
        "mse_coeff = 0  #@param {type: \"number\"}\n",
        "#@markdown image prompt plane, see fov_plane\n",
        "mse_fov = 0.5  #@param {type: \"number\"}\n",
        "#@markdown render a separate view for the image prompt\n",
        "mse_single = False #@param {type: \"boolean\"}\n",
        "#@markdown background of the view\n",
        "mse_bg = 1.0 #@param {type: \"number\"}\n",
        "#@markdown L2 regularization. reg_color: regularize RGB apart from density?\n",
        "reg_coeff = 3  #@param {type: \"number\"}  # 1\n",
        "reg_color = True  #@param {type: \"boolean\"}\n",
        "#@markdown TV regularization, increase this to 4 or 5 to make the image smoother if it is too noisy\n",
        "tv_coeff =   3#@param {type: \"number\"}  # 1050\n",
        "#@markdown CLIP weight\n",
        "clip_coeff = 20  #@param {type: \"number\"}  # 4\n",
        "#@markdown CLIP loss type, might improve the results\n",
        "loss_type = \"cosine\"  #@param [\"spherical\", \"cosine\"] {type: \"string\"}\n",
        "#@markdown spherical regularization coefficient. making this higher \"shrinks\" the shape. this is preferred for making the image more coherent over tau_coeff\n",
        "spherical_coeff =    35#@param {type: \"number\"}  # 100\n",
        "#@markdown weighting for size of the virtual sphere. raise this to make the shape a little bigger\n",
        "sphere_size =   20#@param {type: \"number\"}\n",
        "#@markdown tau regularization from dream fields. tau_target limits the shape's visual size and the coefficient while tau_coeff makes the shape disappear faster\n",
        "tau_coeff =   5#@param {type: \"number\"}\n",
        "tau_target = 0.5  #@param {type: \"number\"}  # 0.18  # 0.25  # 0.2 * 0.5\n",
        "#@markdown ranges for random azimuth, altitude and offset shifts (augmentations)\n",
        "shuffle_ang =   10#@param {type: \"number\"}  # 1.28  # 10  # 0.1\n",
        "shuffle_altitude = 0.0  #@param {type: \"number\"}  # 0.1\n",
        "shuffle_offset =   0.1#@param {type: \"number\"}\n",
        "shuffle_xy =   0.1#@param {type: \"number\"}  # 1.28  # 10  # 0.1\n",
        "first_for = 3  #@param {type: \"number\"}\n",
        "#@markdown ## timing settings\n",
        "#@markdown how long to spin before training, showing the empty cube\n",
        "spin_before =   30#@param {type: \"integer\"}\n",
        "#@markdown how long to spin after training\n",
        "spin_length = 60  #@param {type: \"integer\"}\n",
        "#@markdown how many spins to perform after training\n",
        "spin_number = 2  #@param {type: \"integer\"}\n",
        "#@markdown number of frames to show the still picture for\n",
        "still_frames = 30  #@param {type: \"integer\"}\n",
        "#@markdown how many spins to perform while training\n",
        "train_spins = 1  #@param {type: \"integer\"}\n",
        "#@markdown rotate the visualization while training?\n",
        "do_rotate = True  #@param {type: \"boolean\"}\n",
        "#@markdown skip training when visualizing?\n",
        "only_spin = False  #@param {type: \"boolean\"}\n",
        "#@markdown show how the scene actually looks like or debug images?\n",
        "spin_quality = True  #@param {type: \"boolean\"}\n",
        "#@markdown display the image prompt?\n",
        "display_img = False  #@param {type: \"boolean\"}\n",
        "#@markdown raise an error after modifying settings (useful for debugging)\n",
        "stop_after_settings = False  #@param {type: \"boolean\"}\n",
        "#@markdown keep adding to the same video on every run. this is recommended for continuing an interrupted run\n",
        "same_video = False  #@param {type: \"boolean\"}\n",
        "#@markdown continue the optimization you stopped in the middle. this is recommended for continuing an interrupted run\n",
        "same_color = False  #@param {type: \"boolean\"}\n",
        "#@markdown note: you don't need these options if you're already loading from a checkpoint\n",
        "\n",
        "\n",
        "try:\n",
        "    frames\n",
        "except NameError:\n",
        "    frames = []\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if not seed:\n",
        "    seed = random.getrandbits(8)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "# fetch function from a diffusion notebook\n",
        "# used for geting image prompts and palettes\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "# shape rotator. moves and offsets rays\n",
        "def prepare(xyd, offset_z, angle_x, angle_y=0, offset_x=0, offset_y=0, batch=1):\n",
        "    tensorize = lambda x: x.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) if isinstance(x, torch.Tensor) else x\n",
        "    xyd = xyd.repeat(batch, 1, 1, 1, 1)\n",
        "        \n",
        "    offset_x = tensorize(offset_x)\n",
        "    offset_y = tensorize(offset_y)\n",
        "    offset_z = tensorize(offset_z)\n",
        "    \n",
        "    xyd[..., 0] += offset_x\n",
        "    xyd[..., 1] += offset_y\n",
        "    xyd[..., 2] -= offset_z\n",
        "    \n",
        "    a = torch.atan2(xyd[..., 2], xyd[..., 1])\n",
        "    f = (xyd[..., [1, 2]] ** 2).sum(dim=-1) ** 0.5\n",
        "    angle_y = tensorize(angle_y)\n",
        "    a += angle_y\n",
        "    xyd = torch.stack((xyd[..., 0], torch.cos(a) * f, torch.sin(a) * f), dim=-1)\n",
        "    \n",
        "    a = torch.atan2(xyd[..., -1], xyd[..., 0])\n",
        "    f = (xyd[..., [0, -1]] ** 2).sum(dim=-1) ** 0.5\n",
        "    angle_x = tensorize(angle_x)\n",
        "    a += angle_x\n",
        "    xyd = torch.stack((torch.cos(a) * f, xyd[..., 1], torch.sin(a) * f), dim=-1)\n",
        "    \n",
        "    return xyd\n",
        "\n",
        "\n",
        "# clamp with gradients\n",
        "def cl(x):\n",
        "    return torch.relu(1 - torch.relu(1 - x))\n",
        "\n",
        "\n",
        "# simple 3D voxel renderer. very inefficient, no filtering\n",
        "@torch.jit.script\n",
        "def render(color, xyd,\n",
        "           extent: float = extent,\n",
        "           bg_color: float = bg_color,\n",
        "           use_weights: bool = use_weights,\n",
        "           mask_value: float = mask_value,\n",
        "           return_depth: bool = False):\n",
        "    color = torch.nn.functional.pad(color[1:-1, 1:-1, 1:-1],\n",
        "                                    (0, 0, 1, 1, 1, 1, 1, 1),\n",
        "                                    value=mask_value)\n",
        "    color = cl(color)\n",
        "    # idk how to do bilinear interpolation in 3D so this entire section is just that\n",
        "    with torch.no_grad():\n",
        "        xyd = xyd / 2 / extent + 0.5\n",
        "        xyd = xyd.clamp(0, 1)\n",
        "        xyd *= torch.tensor(color.shape[:-1]).to(color.device) - 1\n",
        "        rounds = [xyd]\n",
        "        weights = [torch.ones_like(xyd[..., -1])]\n",
        "        for dim in range(xyd.shape[-1] * int(use_weights)):\n",
        "            new_rounds = []\n",
        "            new_weights = []\n",
        "            for r, m in zip(rounds, weights):\n",
        "                d = r[..., dim] - r[..., dim].floor()\n",
        "                r1 = r.clone()\n",
        "                r1[..., dim] = r[..., dim].floor()\n",
        "                new_rounds.append(r1)\n",
        "                new_weights.append((1 - d) * m)\n",
        "                r2 = r.clone()\n",
        "                r2[..., dim] = r[..., dim].ceil()\n",
        "                new_rounds.append(r2)\n",
        "                new_weights.append(d * m)\n",
        "            rounds = new_rounds\n",
        "            weights = new_weights\n",
        "        rounds = torch.stack(rounds, dim=-2).long()\n",
        "        weights = torch.stack(weights, dim=-1)\n",
        "        for t in range(rounds.shape[-1]):  # [2, 1]\n",
        "            rounds[..., :t] *= color.shape[t]\n",
        "        rounds = rounds.sum(dim=-1)\n",
        "    # this is the actual renderer\n",
        "    color = color.view((-1, color.shape[-1]))[rounds.ravel(), :].view(rounds.shape + (color.shape[-1],))\n",
        "    color = color * weights[..., None]\n",
        "    color = color.sum(dim=-2)\n",
        "    density = color[..., -1]\n",
        "    lg = torch.cat((density[..., :1] * 0, torch.log(1 - density[..., 1:])), dim=-1)\n",
        "    resid = torch.exp(torch.cumsum(lg, dim=-1))\n",
        "    density = density * resid\n",
        "    rgb = (color * density[..., None]).sum(dim=-2) + bg_color * (1 - density.sum(dim=-1))[..., None]\n",
        "    if not return_depth:\n",
        "        return rgb, density.sum(dim=-1)\n",
        "    else:\n",
        "        return rgb, density\n",
        "\n",
        "\n",
        "# total variation regularizer\n",
        "def tv(x):\n",
        "    return ((x[1:] - x[:-1]) ** 2).mean() + ((x[:, 1:] - x[:, :-1]) ** 2).mean() + ((x[:, :, 1:] - x[:, :, :-1]) ** 2).mean()\n",
        "\n",
        "\n",
        "# blur the image for free pyramids\n",
        "def interpolate(color, scale_from=scale_from, scale_decay=scale_decay, grayscale=grayscale):\n",
        "    new_color = color.permute(3, 0, 1, 2).unsqueeze(0)\n",
        "    res = torch.zeros_like(new_color)\n",
        "    s = block_size / (2 ** scale_from)\n",
        "    p = 1 / (2 ** scale_from)\n",
        "    j = 0\n",
        "    total = 0\n",
        "    while s > 0:\n",
        "        scale = 2 ** (scale_decay * j)\n",
        "        res = res + scale * torch.nn.functional.interpolate(\n",
        "            torch.nn.functional.interpolate(\n",
        "                new_color, scale_factor=p, mode=\"trilinear\", align_corners=False),\n",
        "                size=new_color.shape[-3:], mode=\"trilinear\", align_corners=False)\n",
        "        total += scale\n",
        "        s, p, j = s // 2, p / 2, j + 1  # binary pyramid\n",
        "    res = res / total\n",
        "    res = res[0].permute(1, 2, 3, 0)\n",
        "    if grayscale:\n",
        "        res = torch.cat((torch.stack((res[..., :-1].mean(dim=-1),) * 3, dim=-1), res[..., -1:]), dim=-1)\n",
        "    if palette is not None:\n",
        "        colors = res[..., :3]\n",
        "        with torch.inference_mode():\n",
        "            palette_dist = ((colors[..., None, :] - palette) ** 2).sum(dim=-1)\n",
        "            colors_chosen = palette_dist.argmin(dim=-1)\n",
        "            del palette_dist\n",
        "            new_colors = palette[colors_chosen]\n",
        "            del colors_chosen\n",
        "            shift = new_colors - colors\n",
        "            del new_colors\n",
        "        # z+q trick from pixray\n",
        "        colors = colors + shift\n",
        "        del shift\n",
        "        res = torch.cat((colors, res[..., 3:]), dim=-1)\n",
        "    return res\n",
        "\n",
        "\n",
        "def setup():\n",
        "    # bad but it works\n",
        "    global color, src_array, out_path, xyd, src, mse_coeff, mse_single, palette\n",
        "    \n",
        "    if os.path.exists(\"source.png\"):\n",
        "        src = Image.open(\"source.png\").resize((w, w)).convert(\"RGB\")\n",
        "        print(\"Image prompt:\")\n",
        "        display(src)\n",
        "        src_array = torch.from_numpy(np.asarray(src) / 255).to(device)\n",
        "    else:\n",
        "        src_array = torch.zeros((w, w, 3), device=device)\n",
        "        mse_coeff = 0.0\n",
        "        mse_single = False\n",
        "    if os.path.exists(\"palette.png\"):\n",
        "        palette = (Image.open(\"palette.png\")\n",
        "                        .convert(\"RGB\"))\n",
        "        palette = palette.resize((int(w * pallete.size[0] / palette.size[1]),\n",
        "                                  w) if palette.size[1] > palette.size[0] else (\n",
        "                                      w, int(w * palette.size[1] / palette.size[0])))\n",
        "        print(\"Palette:\")\n",
        "        display(palette)\n",
        "        palette = np.asarray(palette).reshape((-1, 3)) / 255\n",
        "        from sklearn.cluster import KMeans\n",
        "        palette = KMeans(n_clusters=palette_colors\n",
        "                                ).fit(palette).cluster_centers_\n",
        "        palette = torch.from_numpy(palette).to(device)\n",
        "    else:\n",
        "        palette = None\n",
        "    with torch.no_grad():\n",
        "        y, x = torch.meshgrid(((torch.arange(w, device=device) / w * 2 - 1) * fov_plane,) * 2)\n",
        "        z = torch.linspace(near, far, rendering_steps, device=device)\n",
        "        xy = torch.stack((x, y), dim=-1)\n",
        "        d = z.unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n",
        "        xyd = torch.cat((xy\n",
        "                            .unsqueeze(-2).unsqueeze(0)\n",
        "                            .repeat(1, 1, 1, d.shape[-2], 1),\n",
        "                            torch.ones_like(d).unsqueeze(0)\n",
        "                            .repeat(1, w, w, 1, 1)\n",
        "    ), dim=-1) * d\n",
        "\n",
        "    frames = []\n",
        "    voxels = torch.stack(torch.meshgrid(*((torch.arange(block_size, device=device) / block_size * 2 - 1,) * 3)), dim=-1)\n",
        "    color = torch.cat((voxels, voxels[..., -1:]), dim=-1).clone()\n",
        "    color[..., :3] = torch.rand_like(color[..., :3])\n",
        "    if init_type == \"uniform\":\n",
        "        color[..., -1] = torch.ones_like(color[..., -1]) ** init_pow\n",
        "    elif init_type == \"random\":\n",
        "        color[..., -1] = torch.rand_like(color[..., -1]) ** init_pow\n",
        "    elif init_type == \"spherical\":\n",
        "        color[..., -1] = 1 - ((color[..., :3] ** 2).mean(dim=-1) ** 0.5) ** init_pow\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Init type {init_type} not supported\")\n",
        "    color[..., -1] /= color[..., -1].max()\n",
        "    color[..., -1] *= init_density\n",
        "    color = torch.nn.Parameter(color.detach(), requires_grad=True)\n",
        "\n",
        "\n",
        "# forward renderer\n",
        "def spin(length=spin_length, total=None, start=0,\n",
        "         progress_bar=True, clear=True, show=True, ignore_err=True,\n",
        "         spins=1, interpolate=interpolate,\n",
        "         return_depth=False, return_coords=False):\n",
        "    if total is None:\n",
        "        total = length\n",
        "    if show:\n",
        "        out = Output()\n",
        "        display(out)\n",
        "    try:\n",
        "        angles = list(range(start, start+length))\n",
        "        tq = tqdm if progress_bar else lambda x: x\n",
        "        for i in tq(list(chunked(angles, 4))):\n",
        "            i = torch.tensor(i, dtype=torch.float32, device=device)\n",
        "            i *= np.pi * 2 * spins / total\n",
        "            with torch.inference_mode():\n",
        "                res = interpolate(color)\n",
        "                coords = prepare(xyd, offset, i, batch=len(i))\n",
        "                pics, depths = render(\n",
        "                    res,\n",
        "                    coords,\n",
        "                    return_depth=return_depth)\n",
        "                pics = pics.cpu().numpy()\n",
        "                depths = depths.cpu().numpy()\n",
        "                coords = coords.cpu().numpy()\n",
        "            for pic, depth, coord in zip(pics, depths, coords):\n",
        "                img = Image.fromarray((pic[..., :3] * 255).astype(np.uint8))\n",
        "                img = img,\n",
        "                if return_depth:\n",
        "                    img = img + (depth,)\n",
        "                if return_coords:\n",
        "                    img = img + (coord,)\n",
        "                if len(img) == 1:\n",
        "                    img = img[0]\n",
        "                yield img\n",
        "            \n",
        "            if show:\n",
        "                with out:\n",
        "                    if clear:\n",
        "                        clear_output(wait=True)\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.imshow(pics[0, ..., :3])\n",
        "                    plt.show()\n",
        "    except KeyboardInterrupt:\n",
        "        if ignore_err:\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "\n",
        "def cutout(img, w=int(cutout_w)):\n",
        "    y = random.randint(0, img.shape[0] - w)\n",
        "    x = random.randint(0, img.shape[1] - w)\n",
        "    return img[y:y+w, x:x+w]\n",
        "\n",
        "\n",
        "# trainer\n",
        "def train(text=text, frames=frames):\n",
        "    global it, bar\n",
        "    out = Output()\n",
        "    display(out)\n",
        "    with torch.no_grad():\n",
        "        txt_emb = model.encode_text(clip.tokenize(text).to(device))\n",
        "        txt_emb = torch.nn.functional.normalize(txt_emb, dim=-1)\n",
        "    params = [color]\n",
        "    try:\n",
        "        optimizer_class = getattr(torch.optim, optimizer_name)\n",
        "    except AttributeError:\n",
        "         optimizer_class = getattr(torch_optimizer, optimizer_name)\n",
        "    optimizer = optimizer_class(params, lr)\n",
        "    # torch.optim.Adam(params, lr=lr)\n",
        "    bar = trange(train_steps)\n",
        "    loss_acc = 0\n",
        "    acc_n = 0\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        for it in bar:\n",
        "            rot = (torch.randn(train_batch, device=device)) * shuffle_ang\n",
        "            rot_y = (torch.randn(train_batch, device=device)) * shuffle_altitude\n",
        "            offset_x = torch.randn(train_batch, device=device) * shuffle_xy\n",
        "            offset_y = torch.randn(train_batch, device=device) * shuffle_xy\n",
        "            res = interpolate(color)\n",
        "            with torch.cuda.amp.autocast(enabled=fp16):\n",
        "                img, d = render(res,  # color,\n",
        "                    prepare(xyd, offset\n",
        "                            + torch.rand(train_batch, device=device) * shuffle_offset,\n",
        "                            rot, rot_y, offset_x=offset_x, offset_y=offset_y,\n",
        "                            batch=train_batch), bg_color=0)\n",
        "            img = img[..., :3]\n",
        "            d = d.unsqueeze(-1)\n",
        "            back = torch.zeros_like(img)\n",
        "            s = back.shape\n",
        "            for i in range(s[0]):\n",
        "                for j in range(s[-1]):\n",
        "                    n = random.choice([7, 14, 28])\n",
        "                    back[i, ..., j] = rand_perlin_2d_octaves(s[1:-1], (n, n)).clip(-0.5, 0.5) + 0.5\n",
        "            img = img + back * (1 - d)\n",
        "            pics = img.detach().cpu().numpy()\n",
        "            if not spin_quality:\n",
        "                for pic in pics:\n",
        "                    frames.append(Image.fromarray((pic * 255).astype(np.uint8)))\n",
        "            \n",
        "            \n",
        "            if mse_single:\n",
        "                xyd_ = prepare(xyd, offset, 0, batch=1)\n",
        "                img_mse, _ = render(res,\n",
        "                                    torch.cat((xyd_[..., :2] * mse_fov,\n",
        "                                            xyd_[..., 2:]), dim=-1),\n",
        "                                    bg_color=mse_bg)\n",
        "            else:\n",
        "                img_mse = img.clone()\n",
        "\n",
        "            with out:\n",
        "                clear_output(wait=True)\n",
        "                plt.plot(losses)\n",
        "                plt.show()\n",
        "                if not spin_quality:\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.imshow(pics[0, ..., :3])\n",
        "                    plt.show()\n",
        "                else:\n",
        "                    frames += list(spin(total=len(bar),\n",
        "                                        start=it * int(do_rotate), length=1,\n",
        "                                        progress_bar=False, clear=False,\n",
        "                                        spins=train_spins))\n",
        "                if mse_single and mse_coeff:\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.imshow(img_mse[0, ..., :3].detach().cpu().numpy())\n",
        "                    plt.show()\n",
        "            img_clip = torch.stack([cutout(x) for x in img], dim=0).permute(0, 3, 1, 2)\n",
        "            img_clip = torchvision.transforms.Normalize(\n",
        "                (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))(img_clip)\n",
        "            img_emb = model.encode_image(img_clip)\n",
        "            img_emb = torch.nn.functional.normalize(img_emb, dim=-1)\n",
        "            img_emb[torch.isnan(img_emb)] = 0\n",
        "            txt_emb[torch.isnan(txt_emb)] = 0\n",
        "            img_emb[torch.isinf(img_emb)] = 0\n",
        "            txt_emb[torch.isinf(txt_emb)] = 0\n",
        "\n",
        "            x, y, z = torch.meshgrid(*(((torch.arange(block_size) - block_size // 2) / (block_size // 2),) * 3))\n",
        "            x, y, z = x * sphere_size, y * sphere_size, z * sphere_size\n",
        "            sphere = (x ** 2 + y ** 2 + z ** 2).unsqueeze(0).repeat(train_batch, 1, 1, 1).to(device)\n",
        "            sphere = sphere / sphere.max()\n",
        "            \n",
        "            spherical_loss = (sphere * (color[..., -1] ** 2) * torch.sign(color[..., -1])).mean()\n",
        "            if loss_type == \"spherical\":\n",
        "                clip_loss = (img_emb - txt_emb).norm(dim=-1).div(2).arcsin().pow(2).mul(2).mean()\n",
        "            elif loss_type == \"cosine\":\n",
        "                clip_loss = (1 - img_emb @ txt_emb.T).mean()\n",
        "            else:\n",
        "                raise NotImplementedError(f\"CLIP loss type not supported: {loss_type}\")\n",
        "            mse_loss = ((img_mse[..., :3] - src_array.unsqueeze(0)) ** 2).mean()\n",
        "            reg_loss = ((color if reg_color else color[..., -1:]) ** 2).mean()\n",
        "            tv_loss = tv(res)\n",
        "            tau_loss = d.mean().clamp(tau_target, 100)\n",
        "            loss = (\n",
        "                mse_loss * mse_coeff +\n",
        "                reg_loss * reg_coeff + \n",
        "                tv_loss * tv_coeff + \n",
        "                clip_loss * clip_coeff +\n",
        "                tau_loss * tau_coeff +\n",
        "                spherical_loss * spherical_coeff)\n",
        "            loss.backward()\n",
        "            for param in params:\n",
        "                param.grad.data[torch.isnan(param.grad.data)] = 0\n",
        "                param.grad.data[torch.isinf(param.grad.data)] = 0\n",
        "            loss_acc += loss.item()\n",
        "            acc_n += 1\n",
        "            acc_n += 1\n",
        "            bar.set_description(f\"loss: {loss_acc / max(acc_n, 1)}\"\n",
        "                                f\" mse: {mse_loss.item()} reg: {reg_loss.item()}\"\n",
        "                                f\" tv: {tv_loss.item()} clip: {clip_loss.item()}\"\n",
        "                                f\" tau: {tau_loss.item()} spherical: {spherical_loss.item()}\")\n",
        "            if it % grad_acc == grad_acc - 1:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                loss_acc /= grad_acc\n",
        "                losses.append(loss_acc)\n",
        "                loss_acc = 0\n",
        "                acc_n = 0\n",
        "            if time_stop > 0 and time.time() - start_time > time_stop:\n",
        "                raise KeyboardInterrupt\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "\n",
        "def main():\n",
        "    global text, load_checkpoint, out_path, frames, rgba, save, color\n",
        "    if stop_after_settings:\n",
        "        print(\"Early exit (stop_after_settings=True)\")\n",
        "        return\n",
        "\n",
        "    if capitalize:\n",
        "        text = text[:1].upper() + text[1:]\n",
        "    if capitalize_words:\n",
        "        text = ' '.join(word[:1].upper() + word[1:] for word in text.split())  # TODO\n",
        "    if capitalize_number:  # TODO\n",
        "        text = ' '.join(word.upper() if word[0].isdigit() else word for word in text.split())\n",
        "    print(text)\n",
        "    if not same_video:\n",
        "        frames = []\n",
        "    try:\n",
        "        frames\n",
        "    except:\n",
        "        frames = []\n",
        "    # starting frame\n",
        "    if img_path:\n",
        "        Image.open(fetch(img_path)).save(\"source.png\")\n",
        "    if palette_path:\n",
        "        Palette = Image.open(fetch(palette_path)).save(\"palette.png\")\n",
        "    if not same_color:\n",
        "        setup()\n",
        "    if display_img:\n",
        "        try:\n",
        "            src\n",
        "        except:\n",
        "            setup()\n",
        "        frames = [src] * int(fps * first_for)\n",
        "    try:\n",
        "        color\n",
        "    except:\n",
        "        setup()\n",
        "    if not out_path:\n",
        "        out_path = text + \".mp4\"\n",
        "    if load_checkpoint:\n",
        "        if load_gdrive:\n",
        "            from google.colab import drive\n",
        "            print(\"Mounting drive...\")\n",
        "            drive.mount(\"/content/drive/\")\n",
        "            load_checkpoint = \"/content/drive/MyDrive/\" + load_checkpoint\n",
        "        print(\"Loading checkpoint...\")\n",
        "        try:\n",
        "            save = torch.load(load_checkpoint, map_location=\"cpu\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {load_checkpoint}\")\n",
        "            if ignore_err:\n",
        "                print(\"Starting training from scratch...\")\n",
        "            else:\n",
        "                raise\n",
        "        else:\n",
        "            if \"color\" in save:\n",
        "                print(\"Color loaded\")\n",
        "                color = torch.nn.Parameter(save[\"color\"].to(device).detach())\n",
        "                if \"rgba\" in save:\n",
        "                    rgba = save[\"rgba\"].detach()\n",
        "            elif \"rgba\" in save:\n",
        "                print(\"No color, training functions disabled. PLY export is available\")\n",
        "                rgba = save[\"rgba\"]\n",
        "                1/0\n",
        "            else:\n",
        "                print(\"No RGBA, voxel loading is available (not implemented yet)\")\n",
        "                print(\"Starting training from scratch...\")\n",
        "    print(\"First spin\")\n",
        "    frames += list(spin(spin_before))\n",
        "    print(\"Training\")\n",
        "    if not only_spin:\n",
        "        train(text=text, frames=frames)\n",
        "    else:\n",
        "        frames = []\n",
        "    print(\"Last spin\")\n",
        "    try:\n",
        "        it\n",
        "        bar\n",
        "    except NameError:\n",
        "        it = 0\n",
        "        bar = [0]\n",
        "    frames += list(spin(spin_length*spin_number, total=spin_length*spin_number,\n",
        "                        start=ceil(it/len(bar)*spin_length)+1,\n",
        "                        spins=spin_number))\n",
        "    frames += [frames[-1]] * still_frames\n",
        "\n",
        "    # auto rename\n",
        "    if os.path.exists(out_path) and rename_out:\n",
        "        i = 1\n",
        "        orig_path = out_path\n",
        "        while os.path.exists(out_path):\n",
        "            out_path = f\"{orig_path.rpartition('.')[0]} ({i}).mp4\"\n",
        "            i += 1\n",
        "    \n",
        "    print(\"Saving video\")\n",
        "    p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', out_path], stdin=PIPE)\n",
        "    for im in tqdm(frames):\n",
        "        im.save(p.stdin, 'PNG')\n",
        "    p.stdin.close()\n",
        "    p.wait()\n",
        "    mp4 = open(out_path, \"rb\").read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width={w} controls>\n",
        "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "    if download:\n",
        "        from google.colab import files\n",
        "        files.download(out_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "#@markdown if it runs out of memory, use Runtime -> Restart Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrmIm2VTVZ63",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title convert to point cloud\n",
        "from IPython.display import clear_output, display\n",
        "from tqdm.auto import trange, tqdm\n",
        "from ipywidgets import Output\n",
        "import random\n",
        "\n",
        "\n",
        "#@markdown note: the conversion is non-deterministic\n",
        "\n",
        "#@markdown seed (zero if you're feeling lucky)\n",
        "seed = 42 #@param {type: \"integer\"}\n",
        "#@markdown how much to fade the shape, removing points.\n",
        "#@markdown raising it increases the file size, but the quality improves.\n",
        "\n",
        "#@markdown the file size with the old export method is 1.7 MB at fade=0.04.\n",
        "fade = 0.1 #@param {type: \"slider\", min: 0, max: 1, step:0.01}\n",
        "\n",
        "#@markdown display the spins every N frames\n",
        "show_every = 6 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown the method to turn the volume into a point cloud\n",
        "\n",
        "#@markdown spin renders the point cloud from many views and turns it into a mesh. it's faster but uses more memory\n",
        "\n",
        "#@markdown naïve is the method used before. its main advantage is that it's less memory hungry and more stable.\n",
        "\n",
        "#@markdown spin only exports the surface, while naïve has the entire volume. as a result, spin is smaller but naïve might be more useful for some applications\n",
        "export_method = \"spin\"  #@param [\"spin\", \"naïve\"]\n",
        "\n",
        "if not seed:\n",
        "    seed = random.getrandbits(32)\n",
        "print(\"Fade:\", fade)\n",
        "print(\"Seed:\", seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "vertices = []\n",
        "\n",
        "if export_method == \"spin\":\n",
        "    out = Output()\n",
        "    display(out)\n",
        "    \n",
        "    rgbs = []\n",
        "    ds = []\n",
        "    try:\n",
        "        for i, (rgb, depth, coords) in enumerate(spin(return_depth=True, return_coords=True)):\n",
        "            rgbs.append(rgb)\n",
        "            opacity = depth.sum(axis=-1)\n",
        "            opacity = opacity / opacity.max()\n",
        "            opaque = opacity > 0.05\n",
        "            opaque = opaque * (np.random.rand(*opaque.shape) < fade)\n",
        "            \n",
        "            weights = np.exp(depth)\n",
        "            weights = weights / weights.sum(axis=-1)[..., None]\n",
        "            # zs = (weights * np.arange(0, depth.shape[-1])[None, None, :]).sum(axis=-1).astype(np.int64)\n",
        "            zs = weights.argmax(axis=-1)\n",
        "            ds.append(Image.fromarray(zs.astype(np.uint8)))\n",
        "            # opaque = opaque * (z > 5) * (z < z.max() - 5)\n",
        "            # ((h, w, 3), (h, w)) -> (h, w, 1, 3) -> (h, w, 3)\n",
        "            xyzs = np.take_along_axis(coords, zs[..., None, None], axis=-2)[:, :, 0, :]\n",
        "            \n",
        "            yx = np.stack(np.meshgrid(*[np.arange(w) for w in xyzs.shape[:2]]), axis=-1)\n",
        "            nz, = opaque.flatten().nonzero()\n",
        "            yx_nz = yx.reshape((-1, 2))[nz]\n",
        "\n",
        "            for y, x in yx_nz:\n",
        "                (x, y, z), (r, g, b) = xyzs[y, x], rgb.getpixel((int(y), int(x)))\n",
        "                if any((a < -extent or a > extent) for a in (x, y, z)):\n",
        "                    continue\n",
        "                vertices.append((x, y, z, r / 255, g / 255, b / 255, 1))\n",
        "\n",
        "            if i % show_every == 0:\n",
        "                with out:\n",
        "                    clear_output()\n",
        "                    plt.hist(\n",
        "                        zs.ravel()[nz],\n",
        "                        bins=20)\n",
        "                    plt.show()\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.imshow(zs)\n",
        "                    plt.colorbar()\n",
        "                    plt.show()\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.imshow(1 - (xyzs / 2 + 1))\n",
        "                    plt.show()\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "elif export_method == \"naïve\":  # :)\n",
        "    with torch.inference_mode():\n",
        "        try:\n",
        "            rgba\n",
        "        except NameError:\n",
        "            rgba = interpolate(color)\n",
        "\n",
        "        chosen = (torch.rand_like(rgba[..., 3]) < (rgba[..., 3] * fade)).long().cpu()\n",
        "        xyz = torch.stack(torch.meshgrid([torch.arange(0, d) for d in chosen.shape]), dim=-1)\n",
        "    \n",
        "    try:\n",
        "        bar = trange(rgba.shape[0])\n",
        "        for x in bar:\n",
        "            for y in range(rgba.shape[1]):\n",
        "                for z in range(rgba.shape[2]):\n",
        "                    if not chosen[x, y, z].item():\n",
        "                        continue\n",
        "                    vertices.append((x / rgba.shape[0], y / rgba.shape[1], z / rgba.shape[2])\n",
        "                                    + tuple(max(0, min(1, 0.5 + i.item())) for i in rgba[x, y, z]))\n",
        "            bar.set_description(f\"Vertices: {len(vertices)}, \"\n",
        "                                f\"expected: {int(len(vertices) / (x+1) * rgba.shape[0])}\")\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "print(\"Total:\", len(vertices), \"vertices.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (Only if you turn on spin) export RGB/depth videos\n",
        "from IPython.display import display, HTML\n",
        "from subprocess import Popen, PIPE\n",
        "from base64 import b64encode\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "def save_video(frames, out_path, fps=fps):\n",
        "    p = Popen([\"ffmpeg\", \"-y\", \"-f\", \"image2pipe\", \"-vcodec\", \"png\", \"-r\", str(fps), \"-i\", \"-\", \"-vcodec\", \"libx264\", \"-r\", str(fps), \"-pix_fmt\", \"yuv420p\", \"-crf\", \"17\", \"-preset\", \"veryslow\", out_path], stdin=PIPE)\n",
        "    for im in tqdm(frames):\n",
        "        im.save(p.stdin, \"PNG\")\n",
        "    p.stdin.close()\n",
        "    p.wait()\n",
        "    mp4 = open(out_path, \"rb\").read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width={w} controls>\n",
        "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "\n",
        "try:\n",
        "    save_video(rgbs, \"rgbs.mp4\")\n",
        "    save_video(ds, \"ds.mp4\")\n",
        "except NameError:\n",
        "    pass"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DiG_tBKNsvZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaIVP6qEGT8H",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title export to PLY\n",
        "#@markdown name of the file, leave empty to name automatically\n",
        "file_path = \"\"  #@param [\"\", \"model.ply\"] {type: \"string\", allow-input: true}\n",
        "#@markdown path to save in google drive, leave empty if you want to store it in the colab session\n",
        "drive_path = \"text2voxels\"  #@param [\"\", \"text2voxels\"] {type: \"string\", allow-input: true}\n",
        "if not file_path:\n",
        "    file_path = out_path.replace(\".mp4\", \".ply\")\n",
        "\n",
        "#@markdown download the resulting .ply file?\n",
        "download = True  #@param {type: \"boolean\"}\n",
        "\n",
        "\n",
        "def save_file(out_path):\n",
        "    with open(out_path, 'w') as out_file:\n",
        "        out_file.write('ply\\n')\n",
        "        out_file.write('format ascii 1.0\\n')\n",
        "        out_file.write(f'element vertex {len(vertices)}\\n') # variable # of vertices\n",
        "        out_file.write('property float x\\n')\n",
        "        out_file.write('property float y\\n')\n",
        "        out_file.write('property float z\\n')\n",
        "        out_file.write('property uchar red\\n')\n",
        "        out_file.write('property uchar green\\n')\n",
        "        out_file.write('property uchar blue\\n')\n",
        "        out_file.write('property uchar alpha\\n')\n",
        "        out_file.write('end_header\\n')\n",
        "        for x, y, z, r, g, b, a in tqdm(vertices):\n",
        "            out_file.write(f\"{x} {rgba.shape[1] - y} {z} {int(r * 255)} {int(g * 255)} {int(b * 255)} {int(a * 255)}\\n\")\n",
        "\n",
        "\n",
        "save_file(file_path)\n",
        "\n",
        "if download:\n",
        "    from google.colab import files\n",
        "    files.download(file_path)\n",
        "\n",
        "if drive_path:\n",
        "    from google.colab import drive\n",
        "    print(\"Mounting drive...\")\n",
        "    drive.mount(\"/content/drive/\")\n",
        "    print(\"Saving to drive...\")\n",
        "    drive_path = f\"/content/drive/MyDrive/{drive_path}\"\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    save_file(f\"{drive_path}/{file_path}\")\n",
        "\n",
        "#@markdown then open this in meshlab and enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAZrCusJ2rPj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title (optional, experimental) saving the intermediate representation\n",
        "#@markdown this is the old export option, but these results might be useful in the future when volume export is added.\n",
        "\n",
        "#@markdown you can also continue an interrupted generation using this\n",
        "\n",
        "#@markdown disabled by default, check `save_color` to enable\n",
        "\n",
        "#@markdown warning: the files take up about 50MB compressed at 128x128x128.\n",
        "\n",
        "#@markdown name of the file, leave empty to name automatically\n",
        "file_path = \"\"  #@param [\"\", \"model.pth\"] {type: \"string\", allow-input: true}\n",
        "#@markdown path to save in google drive, leave empty if you want to store it in the colab session\n",
        "drive_path = \"\"  #@param [\"\", \"text2voxels\"] {type: \"string\", allow-input: true}\n",
        "if not file_path:\n",
        "    file_path = out_path.replace(\".mp4\", \".pth\")\n",
        "#@markdown which parts of the model to save.\n",
        "\n",
        "#@markdown with color it's possible to restore the model, but you won't be able\n",
        "#@markdown to open it outside the notebook. with rgba it might be more portable\n",
        "#@markdown but you won't be able to restore and train for longer\n",
        "save_color = False #@param {type: \"boolean\"}\n",
        "save_rgba = True #@param {type: \"boolean\"}\n",
        "# RGB saving removed\n",
        "save_rgb = False  #@#param {type: \"boolean\"}\n",
        "\n",
        "#@markdown download the resulting .pth file?\n",
        "download = True  #@param {type: \"boolean\"}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    try:\n",
        "        rgba\n",
        "    except NameError:\n",
        "        rgba = interpolate(color)\n",
        "\n",
        "    rgb = rgba[..., :3] * rgba[..., -1:]\n",
        "to_save = {}\n",
        "if save_color:\n",
        "    to_save.update(dict(\n",
        "        color=color,\n",
        "        scale_from=scale_from,\n",
        "        scale_decay=scale_decay,\n",
        "        grayscale=grayscale\n",
        "    ))\n",
        "if save_rgba:\n",
        "    to_save[\"rgba\"] = rgba\n",
        "if save_rgb:\n",
        "    to_save[\"rgb\"] = rgb\n",
        "print(\"Saving...\")\n",
        "torch.save(to_save, file_path)\n",
        "\n",
        "if download:\n",
        "    from google.colab import files\n",
        "    files.download(file_path)\n",
        "\n",
        "if drive_path:\n",
        "    from google.colab import drive\n",
        "    print(\"Mounting drive...\")\n",
        "    drive.mount(\"/content/drive/\")\n",
        "    print(\"Saving to drive...\")\n",
        "    drive_path = f\"/content/drive/MyDrive/{drive_path}\"\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    torch.save(to_save, f\"{drive_path}/{file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vYOLmSJXpFC"
      },
      "source": [
        "the end"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text2voxels v0.2.1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}